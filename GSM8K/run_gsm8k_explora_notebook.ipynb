{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from numpy import linalg\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import torch\n",
    "import re\n",
    "import pickle \n",
    "from tqdm import tqdm\n",
    "\n",
    "random.seed(7)\n",
    "np.random.seed(7)\n",
    "torch.manual_seed(7)\n",
    "\n",
    "import transformers\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCESS_TOKEN = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/2 [00:15<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistralai/Mistral-7B-Instruct-v0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mACCESS_TOKEN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 ./cuda_executable\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n",
      "File \u001b[0;32m~/miniconda3/envs/legos_py39/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/legos_py39/lib/python3.9/site-packages/transformers/modeling_utils.py:262\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m~/miniconda3/envs/legos_py39/lib/python3.9/site-packages/transformers/modeling_utils.py:4319\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4310\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   4312\u001b[0m     (\n\u001b[1;32m   4313\u001b[0m         model,\n\u001b[1;32m   4314\u001b[0m         missing_keys,\n\u001b[1;32m   4315\u001b[0m         unexpected_keys,\n\u001b[1;32m   4316\u001b[0m         mismatched_keys,\n\u001b[1;32m   4317\u001b[0m         offload_index,\n\u001b[1;32m   4318\u001b[0m         error_msgs,\n\u001b[0;32m-> 4319\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   4323\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4325\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4326\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4327\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4330\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4331\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgguf_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4337\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4339\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   4340\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/miniconda3/envs/legos_py39/lib/python3.9/site-packages/transformers/modeling_utils.py:4921\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path, weights_only)\u001b[0m\n\u001b[1;32m   4917\u001b[0m         assign_to_params_buffers \u001b[38;5;241m=\u001b[39m check_support_param_buffer_assignment(\n\u001b[1;32m   4918\u001b[0m             model_to_load, state_dict, start_prefix\n\u001b[1;32m   4919\u001b[0m         )\n\u001b[1;32m   4920\u001b[0m     fixed_state_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_fix_state_dict_keys_on_load(state_dict)\n\u001b[0;32m-> 4921\u001b[0m     error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfixed_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43massign_to_params_buffers\u001b[49m\n\u001b[1;32m   4923\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4925\u001b[0m \u001b[38;5;66;03m# force memory release\u001b[39;00m\n\u001b[1;32m   4926\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m state_dict\n",
      "File \u001b[0;32m~/miniconda3/envs/legos_py39/lib/python3.9/site-packages/transformers/modeling_utils.py:717\u001b[0m, in \u001b[0;36m_load_state_dict_into_model\u001b[0;34m(model_to_load, state_dict, start_prefix, assign_to_params_buffers)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    715\u001b[0m             load(child, state_dict, prefix \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, assign_to_params_buffers)\n\u001b[0;32m--> 717\u001b[0m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43massign_to_params_buffers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43massign_to_params_buffers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;66;03m# Delete `state_dict` so it could be collected by GC earlier. Note that `state_dict` is a copy of the argument, so\u001b[39;00m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;66;03m# it's safe to delete it.\u001b[39;00m\n\u001b[1;32m    720\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m state_dict\n",
      "File \u001b[0;32m~/miniconda3/envs/legos_py39/lib/python3.9/site-packages/transformers/modeling_utils.py:715\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix, assign_to_params_buffers)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    714\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 715\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43massign_to_params_buffers\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/legos_py39/lib/python3.9/site-packages/transformers/modeling_utils.py:715\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix, assign_to_params_buffers)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    714\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 715\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43massign_to_params_buffers\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping similar frames: _load_state_dict_into_model.<locals>.load at line 715 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/legos_py39/lib/python3.9/site-packages/transformers/modeling_utils.py:715\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix, assign_to_params_buffers)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    714\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 715\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43massign_to_params_buffers\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/legos_py39/lib/python3.9/site-packages/transformers/modeling_utils.py:711\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix, assign_to_params_buffers)\u001b[0m\n\u001b[1;32m    709\u001b[0m                     module\u001b[38;5;241m.\u001b[39m_load_from_state_dict(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    710\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 711\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_from_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    714\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/legos_py39/lib/python3.9/site-packages/torch/nn/modules/module.py:2438\u001b[0m, in \u001b[0;36mModule._load_from_state_dict\u001b[0;34m(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\u001b[0m\n\u001b[1;32m   2436\u001b[0m             \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, input_param)\n\u001b[1;32m   2437\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2438\u001b[0m             \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_param\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2439\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m   2440\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mswapping\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_swap_tensors \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcopying\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, token=ACCESS_TOKEN)\n",
    "\n",
    "# CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 ./cuda_executable\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "model = model.to(device)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, torch_dtype=torch.float16, token=ACCESS_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gen resp\n",
    "def get_completion(msg_in):\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"You are a helpful, respectful and honest assistant helping to solve math word problems or tasks requiring reasoning or math, use the Chain-of-Thought methodology by following given examples to explain your step-by-step calculations or logic. Do not generate examples in your answer.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\":\"assistant\",\n",
    "            \"content\": \"I understand.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": msg_in,\n",
    "        }\n",
    "    ]\n",
    "        \n",
    "    prompt = pipeline.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    outputs = pipeline(prompt, max_new_tokens=256, do_sample=True, num_return_sequences=10, temperature=0.5, top_k=10, top_p=1.0)\n",
    "        \n",
    "    out_text = []\n",
    "    for x in range(0, 10):\n",
    "        out_text.append(outputs[x][\"generated_text\"])\n",
    "    return out_text\n",
    "\n",
    "# Self consistency on 10 generated answers\n",
    "def self_con(tmp_list):\n",
    "    ans_list = []\n",
    "    for tmp in tmp_list:\n",
    "        ans = \"\"\n",
    "        if len(tmp.split(\"Final Answer:\"))>0:\n",
    "            ans = tmp.split(\"Final Answer:\")[-1]\n",
    "            ans = ans.split(\"\\n\")[0]\n",
    "            # print(ans)\n",
    "            if \"each\" in ans:  ans = ans.split(\"each\")[0]\n",
    "            if \"=\" in ans: ans = ans.split(\"=\")[-1]\n",
    "            ans = re.sub(r'[^0-9.]',\"\",ans)\n",
    "            if len(ans)>0 and ans[-1]==\".\": ans = ans[:-1]\n",
    "            # print(ans, \"**************\")\n",
    "            try:\n",
    "                float(ans)\n",
    "                ans = round(float(ans))\n",
    "                ans_list.append(ans)\n",
    "            except: pass\n",
    "        # ans_list.append(ans)\n",
    "\n",
    "    # print(ans_list)\n",
    "    d = {}\n",
    "    for i in ans_list:\n",
    "        if i==\"\":\n",
    "            continue\n",
    "        if int(i) in d:\n",
    "            d[int(i)] += 1\n",
    "        else:\n",
    "            d[int(i)] = 1\n",
    "    # print(d)\n",
    "    n = sorted(d.items(), key=lambda x:x[1], reverse=True)\n",
    "    return n\n",
    "\n",
    "# Strip answer from sentence\n",
    "def clean_ans(s):\n",
    "    ans_s = s.split(\"#### \")[1]\n",
    "    ans_s = ans_s.replace(\",\",\"\")\n",
    "    return ans_s\n",
    "\n",
    "def get_prompt(ex):\n",
    "    s = \"\\n\\n\"\n",
    "    s += \"Question:\" + ex[\"question\"]+\"\\n\"\n",
    "    ex[\"answer\"] = re.sub(\"<<.*?>>\", \"\", ex[\"answer\"])\n",
    "    ex[\"answer\"] = ex[\"answer\"].replace(\"#### \", \"Final Answer:\")\n",
    "    s += ex[\"answer\"]\n",
    "    return s\n",
    "\n",
    "def llm_output(user_query):\n",
    "    # results = [get_completion(user_query, api_keys[i], endpoint_urls[i], hard_code_exception=hard_code_exception) for i in range(len(endpoint_urls))]\n",
    "    results = get_completion(user_query)\n",
    "    \n",
    "    return results\n",
    "\n",
    "######################################################################################################\n",
    "\n",
    "# Deleted Fsbd search code\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "def prompt_for_manual_prediction(ex, shots):\n",
    "\n",
    "    prompt = \"Follow given examples and solve the Test Question at end in similar manner by giving step by step reasoning followed by the Final Answer.\\n\\n\"\n",
    "    for index, s in shots.iterrows():\n",
    "        prompt += get_prompt(s)\n",
    "\n",
    "    prompt += \"\\n\\nFollowing the given examples generate step by step reasoning in Answer and generate Final Answer for the below question.\\n\\n\" \n",
    "    prompt += \"Question:\" + ex[\"question\"]\n",
    "    \n",
    "    return prompt\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "def MAPE(Y_actual,Y_Predicted):\n",
    "    mape = np.mean(np.abs((Y_actual - Y_Predicted)/Y_actual))*100\n",
    "    return mape\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "def LLM_avg_error(exemplars_set, val_data):\n",
    "    # stop_signal = \"\\n\\n\"\n",
    "    error=[]\n",
    "    # increment error if model predicted answer is not equal to the ground truth answer for the test question by passing the exemplars\n",
    "    for exemplars in tqdm(exemplars_set,total=len(exemplars_set),desc=\"LLM Loss Fn Exemplar\"):\n",
    "        matches = 0\n",
    "        mismatches = 0\n",
    "        exnum = 0\n",
    "        # acc_records = []\n",
    "        for index, row in val_data.iterrows():\n",
    "            prompt = prompt_for_manual_prediction(row, exemplars)\n",
    "            tmp_list = llm_output(prompt)\n",
    "\n",
    "            n = self_con(tmp_list)\n",
    "    \n",
    "            ground_truth = int(clean_ans(row[\"answer\"]))\n",
    "\n",
    "            answer = \"\"\n",
    "            maxf = 0\n",
    "            if len(n)==0: answer=\"\"\n",
    "            else: maxf = n[0][1]\n",
    "\n",
    "            for z in n:\n",
    "                if z[1]==maxf:\n",
    "                    if ground_truth==z[0]:\n",
    "                        answer = z[0]\n",
    "\n",
    "            if answer==\"\": \n",
    "                mismatches += 1\n",
    "                if len(n)>0: answer = n[0][0]\n",
    "            else: matches += 1           \n",
    "            \n",
    "            exnum+=1\n",
    "\n",
    "        error.append(mismatches/exnum)\n",
    "\n",
    "    return error\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "def LLM_error_indicator(exemplars_set, val_data):\n",
    "    # stop_signal = \"\\n\\n\"\n",
    "    error=[]\n",
    "    # increment error if model predicted answer is not equal to the ground truth answer for the test question by passing the exemplars\n",
    "    for exemplars in tqdm(exemplars_set,total=len(exemplars_set),desc=\"LLM Error Fn Exemplar\"):\n",
    "        for index, row in val_data.iterrows():\n",
    "            prompt = prompt_for_manual_prediction(row, exemplars)\n",
    "            # print(\"*********************************\")\n",
    "            # print(prompt)\n",
    "            # print(\"*********************************\")\n",
    "            tmp_list = llm_output(prompt)\n",
    "            n = self_con(tmp_list)\n",
    "            \n",
    "            ground_truth = int(clean_ans(row[\"answer\"]))\n",
    "\n",
    "            answer = \"\"\n",
    "            maxf = 0\n",
    "            if len(n)==0: answer=\"\"\n",
    "            else: maxf = n[0][1]\n",
    "\n",
    "            for z in n:\n",
    "                if z[1]==maxf:\n",
    "                    if ground_truth==z[0]:\n",
    "                        answer = z[0]\n",
    "\n",
    "            if answer==\"\": loss = 1\n",
    "            else: loss = 0\n",
    "\n",
    "            error.append(loss)\n",
    "\n",
    "\n",
    "    return error\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "def static_subset_selection(val_data, train_data, k, test_data):\n",
    "\n",
    "    # test_data = test_data[:50]\n",
    "    val_data = val_data[:20]\n",
    "    # test_data = 15, val_data=9, train_data=30-9=21, k=5, L=40, U=10, V=10, L-U=30\n",
    "    # test_emb = torch.Tensor(test_emb)\n",
    "\n",
    "    with open('../datasets/GSM8K/embeddings/transfer_emb.pkl', 'rb') as f:\n",
    "        transfer_emb = pickle.load(f)\n",
    "    \n",
    "    with open('../datasets/GSM8K/embeddings/val_emb.pkl', 'rb') as f1:\n",
    "        val_emb = pickle.load(f1)\n",
    "    \n",
    "    val_emb = val_emb[:20]\n",
    "    \n",
    "    val_emb = torch.Tensor(val_emb)\n",
    "    transfer_emb = torch.Tensor(transfer_emb)\n",
    "\n",
    "    print(\"*****Embeddings loaded*****\")\n",
    "    # k-means clustering on train_data with k=5\n",
    "    kmeans = KMeans(n_clusters=5, random_state=0).fit(transfer_emb)\n",
    "    #print(kmeans.labels_)\n",
    "    # Make clusters of train_data with same cluster label\n",
    "    train_data['cluster'] = kmeans.labels_\n",
    "\n",
    "    # # Do stratified sampling from train_data based on the first word of the question\n",
    "    # train_data['w1'] = train_data['question'].str.split().str[0]\n",
    "\n",
    "    # Create index column in train_data using arange\n",
    "    train_data['index'] = np.arange(len(train_data))\n",
    "\n",
    "    # Create L=_ subsets of size k total, with each group having k/num_gr examples\n",
    "    num_gr = len(train_data['cluster'].unique())\n",
    "    L = []\n",
    "    L_indices = []\n",
    "\n",
    "    # print(\"*****Initializing L*****\")\n",
    "    # Initialize L, 100 random set of subsets from train_data \n",
    "    for i in range(100):\n",
    "        subset = []\n",
    "        for name, group in train_data.groupby('cluster'):\n",
    "            # subset.append(group.sample(k//num_gr))   \n",
    "            subset.append(group.sample(1))   \n",
    "        subsets = pd.concat(subset)\n",
    "        L.append(subsets)\n",
    "        L_indices.append(subsets['index'].tolist())\n",
    "\n",
    "    # print(\"*****Initializing U*****\")\n",
    "    # Initialize U, 15 random set of subsets from L \n",
    "    ind_L = np.arange(0,len(L)).tolist()\n",
    "    \n",
    "    ind_total = random.sample(ind_L, 15)\n",
    "    ind_U = ind_total[:10] #10\n",
    "    ind_V = ind_total[10:] #5\n",
    "\n",
    "    ind_L_minus_U = [x for x in ind_L if x not in ind_U]    \n",
    "\n",
    "    U = []\n",
    "    for i in ind_U:\n",
    "        U.append(L[i])\n",
    "    V = []\n",
    "    for i in ind_V:\n",
    "        V.append(L[i])\n",
    "        \n",
    "    L_minus_U = []\n",
    "    for i in ind_L_minus_U:\n",
    "        L_minus_U.append(L[i])\n",
    "\n",
    "\n",
    "    # Calculate the similarity matrix, Eij = cosine similarity between exemplar x_i=train_data and test example u_j=val_data\n",
    "    E_val = cosine_similarity(transfer_emb, val_emb)\n",
    "    # E_test = cosine_similarity(transfer_emb, test_emb)\n",
    "\n",
    "    print(\"*****Calculating LLM Loss*****\")\n",
    "    # Calculate Loss(Y,S) for all S in U\n",
    "    LLM_loss = LLM_error_indicator(U, val_data)\n",
    "    # LLM_loss_on_test_data = LLM_error_indicator(U, test_data)\n",
    "    LLM_loss_on_L_minus_U = LLM_error_indicator(L_minus_U, val_data)\n",
    "    LLM_loss_on_V = LLM_error_indicator(V, val_data)\n",
    "\n",
    "\n",
    "    approx_error_on_U = []\n",
    "    approx_error_on_U_after_update = []\n",
    "    approx_error_on_L_minus_U = []\n",
    "    approx_error_on_L_minus_U_after_update = []\n",
    "    approx_error_on_V = []\n",
    "    approx_error_on_V_after_update = []\n",
    "\n",
    "    '''\n",
    "    approx_error_on_U_on_test_data = []\n",
    "    approx_error_on_U_after_update_on_test_data = []\n",
    "    approx_error_on_L_minus_U_on_test_data = []\n",
    "    approx_error_on_L_minus_U_after_update_on_test_data = []\n",
    "    approx_error_on_V_on_test_data = []\n",
    "    approx_error_on_V_after_update_on_test_data = []\n",
    "    '''\n",
    "\n",
    "    approx_value_on_U = []\n",
    "    approx_value_on_U_after_update = []\n",
    "    approx_value_on_L_minus_U = []\n",
    "    approx_value_on_L_minus_U_after_update = []\n",
    "    approx_value_on_V = []\n",
    "    approx_value_on_V_after_update = []\n",
    "\n",
    "    '''\n",
    "    approx_value_on_U_on_test_data = []\n",
    "    approx_value_on_U_after_update_on_test_data = []\n",
    "    approx_value_on_L_minus_U_on_test_data = []\n",
    "    approx_value_on_L_minus_U_after_update_on_test_data = []\n",
    "    approx_value_on_V_on_test_data = []\n",
    "    approx_value_on_V_after_update_on_test_data = []\n",
    "    '''\n",
    "\n",
    "    LLM_loss_on_val = []\n",
    "    avg_LLM_loss_on_val = []\n",
    "    min_LLM_loss_on_val = []\n",
    "    max_LLM_loss_on_val = []\n",
    "    LLM_loss_on_val_for_each_subset = np.reshape(LLM_loss, (len(U),-1))\n",
    "    LLM_loss_on_val_for_each_subset = np.average(LLM_loss_on_val_for_each_subset, axis=1)\n",
    "    LLM_loss_on_val_avg = np.average(LLM_loss_on_val_for_each_subset)\n",
    "    LLM_loss_on_val_min = np.min(LLM_loss_on_val_for_each_subset)\n",
    "    LLM_loss_on_val_max = np.max(LLM_loss_on_val_for_each_subset)\n",
    "    LLM_loss_on_val.append(LLM_loss_on_val_for_each_subset.tolist())\n",
    "    avg_LLM_loss_on_val.append(LLM_loss_on_val_avg.tolist()) \n",
    "    min_LLM_loss_on_val.append(LLM_loss_on_val_min.tolist())\n",
    "    max_LLM_loss_on_val.append(LLM_loss_on_val_max.tolist())\n",
    "    print(\"\\n********* LLM LOSS ON U FOR VALIDATION DATA *********\")\n",
    "    print(\"\\nLLM_loss_on_val:\",LLM_loss_on_val)\n",
    "    print(\"AVG_LLM_loss_on_VAL_data:\",avg_LLM_loss_on_val)\n",
    "    print(\"MIN_LLM_loss_on_VAL_data:\",min_LLM_loss_on_val)\n",
    "    print(\"MAX_LLM_loss_on_VAL_data:\",max_LLM_loss_on_val)\n",
    "    #===============================================================================\n",
    "\n",
    "\n",
    "    '''\n",
    "    LLM_loss_on_test = []\n",
    "    avg_LLM_loss = []\n",
    "    min_LLM_loss = []\n",
    "    max_LLM_loss = []\n",
    "    LLM_loss_for_each_subset = np.reshape(LLM_loss_on_test_data, (len(U),-1))\n",
    "    LLM_loss_for_each_subset = np.average(LLM_loss_for_each_subset, axis=1)\n",
    "    LLM_loss_avg = np.average(LLM_loss_for_each_subset)\n",
    "    LLM_loss_min = np.min(LLM_loss_for_each_subset)\n",
    "    LLM_loss_max = np.max(LLM_loss_for_each_subset)\n",
    "    LLM_loss_on_test.append(LLM_loss_for_each_subset.tolist())\n",
    "    avg_LLM_loss.append(LLM_loss_avg.tolist()) \n",
    "    min_LLM_loss.append(LLM_loss_min.tolist())\n",
    "    max_LLM_loss.append(LLM_loss_max.tolist())\n",
    "    print(\"\\n********* LLM LOSS ON U FOR TEST DATA *********\")\n",
    "    print(\"\\nLLM_loss_on_test:\",LLM_loss_on_test)\n",
    "    print(\"AVG_LLM_loss_on_TEST_data:\",avg_LLM_loss)\n",
    "    print(\"MIN_LLM_loss_on_TEST_data:\",min_LLM_loss)\n",
    "    print(\"MAX_LLM_loss_on_TEST_data:\",max_LLM_loss)\n",
    "    '''\n",
    "    #===============================================================================\n",
    "\n",
    "\n",
    "\n",
    "    LLM_loss_on_L_minus_U_on_val = []\n",
    "    avg_LLM_loss_on_L_minus_U_on_val = []\n",
    "    min_LLM_loss_on_L_minus_U_on_val = []\n",
    "    max_LLM_loss_on_L_minus_U_on_val = []\n",
    "    LLM_loss_on_val_for_each_subset = np.reshape(LLM_loss_on_L_minus_U, (len(L_minus_U),-1))\n",
    "    LLM_loss_on_val_for_each_subset = np.average(LLM_loss_on_val_for_each_subset, axis=1)\n",
    "    LLM_loss_on_val_avg = np.average(LLM_loss_on_val_for_each_subset)\n",
    "    LLM_loss_on_val_min = np.min(LLM_loss_on_val_for_each_subset)\n",
    "    LLM_loss_on_val_max = np.max(LLM_loss_on_val_for_each_subset)\n",
    "    LLM_loss_on_L_minus_U_on_val.append(LLM_loss_on_val_for_each_subset.tolist())\n",
    "    avg_LLM_loss_on_L_minus_U_on_val.append(LLM_loss_on_val_avg.tolist()) \n",
    "    min_LLM_loss_on_L_minus_U_on_val.append(LLM_loss_on_val_min.tolist())\n",
    "    max_LLM_loss_on_L_minus_U_on_val.append(LLM_loss_on_val_max.tolist())\n",
    "    print(\"\\n********* LLM LOSS ON L_minus_U FOR VALIDATION DATA *********\")\n",
    "    print(\"\\nLLM_loss_on_val:\",LLM_loss_on_L_minus_U_on_val)\n",
    "    print(\"AVG_LLM_loss_on_VAL_data:\",avg_LLM_loss_on_L_minus_U_on_val)\n",
    "    print(\"MIN_LLM_loss_on_VAL_data:\",min_LLM_loss_on_L_minus_U_on_val)\n",
    "    print(\"MAX_LLM_loss_on_VAL_data:\",max_LLM_loss_on_L_minus_U_on_val)\n",
    "    #===============================================================================\n",
    "\n",
    "    #===============================================================================\n",
    "\n",
    "\n",
    "    LLM_loss_on_V_on_val = []\n",
    "    avg_LLM_loss_on_V_on_val = []\n",
    "    min_LLM_loss_on_V_on_val = []\n",
    "    max_LLM_loss_on_V_on_val = []\n",
    "    LLM_loss_on_val_for_each_subset = np.reshape(LLM_loss_on_V, (len(V),-1))\n",
    "    LLM_loss_on_val_for_each_subset = np.average(LLM_loss_on_val_for_each_subset, axis=1)\n",
    "    LLM_loss_on_val_avg = np.average(LLM_loss_on_val_for_each_subset)\n",
    "    LLM_loss_on_val_min = np.min(LLM_loss_on_val_for_each_subset)\n",
    "    LLM_loss_on_val_max = np.max(LLM_loss_on_val_for_each_subset)\n",
    "    LLM_loss_on_V_on_val.append(LLM_loss_on_val_for_each_subset.tolist())\n",
    "    avg_LLM_loss_on_V_on_val.append(LLM_loss_on_val_avg.tolist()) \n",
    "    min_LLM_loss_on_V_on_val.append(LLM_loss_on_val_min.tolist())\n",
    "    max_LLM_loss_on_V_on_val.append(LLM_loss_on_val_max.tolist())\n",
    "    print(\"\\n********* LLM LOSS ON V FOR VALIDATION DATA *********\")\n",
    "    print(\"\\nLLM_loss_on_val:\",LLM_loss_on_V_on_val)\n",
    "    print(\"AVG_LLM_loss_on_VAL_data:\",avg_LLM_loss_on_V_on_val)\n",
    "    print(\"MIN_LLM_loss_on_VAL_data:\",min_LLM_loss_on_V_on_val)\n",
    "    print(\"MAX_LLM_loss_on_VAL_data:\",max_LLM_loss_on_V_on_val)\n",
    "\n",
    "\n",
    "    #===============================================================================\n",
    "    # Calculate the pairwise overlap between the subsets in U\n",
    "    overlaps=[]\n",
    "    for i in range(len(U)):\n",
    "        inner_overlaps=[]\n",
    "        for j in range(len(U)):\n",
    "            if i!=j:\n",
    "                overlap=0\n",
    "                for index_j, s_i in U[i].iterrows():\n",
    "                    for index_j, s_j in U[j].iterrows():\n",
    "                        if s_i[\"question\"].lower() in s_j[\"question\"].lower() or s_j[\"question\"].lower() in s_i[\"question\"].lower():\n",
    "                            overlap+=1\n",
    "                inner_overlaps.append(overlap)\n",
    "        overlaps.append(inner_overlaps)\n",
    "            \n",
    "    print(\"\\nOverlaps:\",overlaps)\n",
    "    print(\"Len of overlaps:\",len(overlaps))\n",
    "\n",
    "    overlap_for_subset = []\n",
    "    avg_overlap = []\n",
    "    min_overlap = []\n",
    "    max_overlap = []\n",
    "    overlap_for_each_subset = np.average(overlaps, axis=1)\n",
    "    overlap_avg = np.average(overlap_for_each_subset)\n",
    "    overlap_min = np.min(overlap_for_each_subset)\n",
    "    overlap_max = np.max(overlap_for_each_subset)\n",
    "    overlap_for_subset.append(overlap_for_each_subset.tolist())\n",
    "    avg_overlap.append(overlap_avg.tolist()) \n",
    "    min_overlap.append(overlap_min.tolist())\n",
    "    max_overlap.append(overlap_max.tolist())\n",
    "    print(\"\\n********* PAIRWISE OVERLAP *********\")\n",
    "    print(\"\\nOverlap_for_subset:\",overlap_for_subset)\n",
    "    print(\"\\nAVG_overlap:\",avg_overlap)\n",
    "    print(\"MIN_overlap:\",min_overlap)\n",
    "    print(\"MAX_overlap:\",max_overlap)\n",
    "    #===============================================================================\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ################################################################################\n",
    "    # Storing the indices of the subsets in U_t\n",
    "    U_indices = []\n",
    "    for i in range(len(U)):\n",
    "        U_indices.append(U[i]['index'].tolist())\n",
    "\n",
    "    V_indices = []\n",
    "    for i in range(len(V)):\n",
    "        V_indices.append(V[i]['index'].tolist())\n",
    "\n",
    "    L_minus_U_indices = []\n",
    "    for i in range(len(L_minus_U)):\n",
    "        L_minus_U_indices.append(L_minus_U[i]['index'].tolist())\n",
    "\n",
    "\n",
    "    ################################################################################\n",
    "    # fill W = E_{ij} for all i in U_t\n",
    "    l = len(U)\n",
    "    W_val = np.zeros((l*len(val_emb), len(transfer_emb)))\n",
    "    # W_test = np.zeros((l*len(test_emb), len(transfer_emb)))\n",
    "    for u in range(l):\n",
    "        for i in range(len(transfer_emb)):\n",
    "            if i in U_indices[u]:\n",
    "                W_val[u*len(val_emb):(u*len(val_emb)+len(val_emb)),i] = E_val[i]\n",
    "                # W_test[u*len(test_emb):(u*len(test_emb)+len(test_emb)),i] = E_test[i]\n",
    "\n",
    "\n",
    "    ################################################################################\n",
    "    # fill V = E_{ij} for all i in V_t\n",
    "    v = len(V)\n",
    "    V_val = np.zeros((v*len(val_emb), len(transfer_emb)))\n",
    "    #V_test = np.zeros((v*len(test_emb), len(transfer_emb)))\n",
    "    for u in range(v):\n",
    "        for i in range(len(transfer_emb)):\n",
    "            if i in V_indices[u]:\n",
    "                V_val[u*len(val_emb):(u*len(val_emb)+len(val_emb)),i] = E_val[i]\n",
    "                #V_test[u*len(test_emb):(u*len(test_emb)+len(test_emb)),i] = E_test[i]\n",
    "\n",
    "\n",
    "\n",
    "    ##############################################################################\n",
    "    # make X = E_{ij} for all i in L-U_t \n",
    "    L_minus_U_len = len(L)-len(U)\n",
    "    X_val = np.zeros((L_minus_U_len*len(val_emb), len(transfer_emb)))\n",
    "    # X_test = np.zeros((L_minus_U_len*len(test_emb), len(transfer_emb)))\n",
    "    for u in range(L_minus_U_len):\n",
    "        for i in range(len(transfer_emb)):\n",
    "            if i in L_minus_U_indices[u]:\n",
    "                X_val[u*len(val_emb):(u*len(val_emb)+len(val_emb)),i] = E_val[i]\n",
    "                # X_test[u*len(test_emb):(u*len(test_emb)+len(test_emb)),i] = E_test[i]\n",
    "\n",
    "\n",
    "    # *~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*\n",
    "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! WHILE LOOP !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    # *~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*\n",
    "    \n",
    "    # while t<15:\n",
    "    for i in tqdm(range(0, 15), desc=\"Iterating Loop of Static Selection\"):\n",
    "\n",
    "        ################################################################################\n",
    "        # min_alpha_i (LLM_loss-W*alpha)^2\n",
    "        #alpha = np.linalg.lstsq(W_val, LLM_loss, rcond=None)[0]\n",
    "        # increase rows of llm loss by appending llm loss of V\n",
    "        # increase rows of W by appending V\n",
    "\n",
    "        ###################    Storing the values for each iteration    ################\n",
    "\n",
    "        \n",
    "        ################################################################################\n",
    " \n",
    "        LLM_loss_on_U_V = LLM_loss + LLM_loss_on_V\n",
    "        W_V_val = np.concatenate((W_val, V_val), axis=0)\n",
    "\n",
    "        print(\"\\n LLM_loss_on_U_V_len:\",len(LLM_loss_on_U_V))\n",
    "        print(\"\\n LLM_loss_on_U_V:\",LLM_loss_on_U_V)\n",
    "        print(\"\\n W_V_val_shape:\",W_V_val.shape)\n",
    "        print(\"\\n W_V_val:\",W_V_val)\n",
    "\n",
    "        alpha = np.linalg.lstsq(W_V_val, LLM_loss_on_U_V, rcond=None)[0]\n",
    "        # Print alpha\n",
    "\n",
    "        print(\"\\nAlpha shape:\",alpha.shape)\n",
    "        print(\"\\nAlpha:\",alpha)\n",
    "\n",
    "\n",
    "\n",
    "        ################################################################################\n",
    "        # Calculate the worst subset S_worst ∈ U_t that maximizes the approximate loss\n",
    "        mul1 = np.matmul(W_val, alpha)\n",
    "        mul_new1 = np.reshape(mul1, (len(U),-1))\n",
    "        S_worst_ind = np.argmax(np.sum(mul_new1, axis=1))\n",
    "        S_worst = U[S_worst_ind]\n",
    "\n",
    "        ##############################################################################\n",
    "        # mul1_test = np.matmul(W_test, alpha)\n",
    "\n",
    "        ###############################################################################\n",
    "        # Calculate the top set S_star ∈ L \\ U_t that minimizes the approximate loss\n",
    "        mul2 = np.matmul(X_val, alpha)\n",
    "        mul_new2 = np.reshape(mul2, (L_minus_U_len,-1))\n",
    "        S_star_ind = np.argmin(np.sum(mul_new2, axis=1))\n",
    "        S_star = L_minus_U[S_star_ind]\n",
    "\n",
    "        ###############################################################################\n",
    "        mul3 = np.matmul(V_val, alpha)\n",
    "\n",
    "        ###############################################################################\n",
    "\n",
    "        \n",
    "        #********* APPROX VALUE ON U ON VALIDATION DATA *********\n",
    "        approx_value_on_U_for_each_subset = np.reshape(mul1, (len(U),-1))\n",
    "        approx_value_on_U_for_each_subset = np.average(approx_value_on_U_for_each_subset, axis=1)\n",
    "        approx_value_on_U.append(approx_value_on_U_for_each_subset.tolist())\n",
    "\n",
    "        '''\n",
    "        #********* APPROX VALUE ON U ON TEST DATA *********\n",
    "        approx_value_on_U_on_test_for_each_subset = np.reshape(mul1_test, (-1,len(test_data)))\n",
    "        approx_value_on_U_on_test_for_each_subset = np.average(approx_value_on_U_on_test_for_each_subset, axis=1)\n",
    "        approx_value_on_U_on_test_data.append(approx_value_on_U_on_test_for_each_subset.tolist())\n",
    "        '''\n",
    "\n",
    "        #********* APPROX VALUE ON L-U ON VALIDATION DATA *********\n",
    "        approx_value_on_L_minus_U_for_each_subset = np.reshape(mul2, (-1,len(val_data)))\n",
    "        approx_value_on_L_minus_U_for_each_subset = np.average(approx_value_on_L_minus_U_for_each_subset, axis=1)\n",
    "        approx_value_on_L_minus_U.append(approx_value_on_L_minus_U_for_each_subset.tolist())\n",
    "\n",
    "        #********* APPROX VALUE ON V ON VALIDATION DATA *********\n",
    "        approx_value_on_V_for_each_subset = np.reshape(mul3, (-1,len(val_data)))\n",
    "        approx_value_on_V_for_each_subset = np.average(approx_value_on_V_for_each_subset, axis=1)\n",
    "        approx_value_on_V.append(approx_value_on_V_for_each_subset.tolist())\n",
    "\n",
    "\n",
    "        #calculate the approximate error = (LLM_loss-W*alpha) on U\n",
    "        print(\"\\n*************Approximation error of Validation Data on U ************\")\n",
    "        print(\"\\nLLM Loss:\",LLM_loss) \n",
    "        print(\"\\nApproximation:\\n\",mul1)\n",
    "        error1 = np.abs(np.array(LLM_loss) - mul1)\n",
    "        error1 = np.reshape(error1, (-1,len(val_data)))\n",
    "        error1 = np.mean(error1, axis=1)\n",
    "        approx_error_on_U.append(error1.tolist())\n",
    "        print(\"\\nApprox error on U on val data:\",approx_error_on_U) \n",
    "\n",
    "        '''\n",
    "        print(\"\\n*************Approximation error of Test Data on U ************\")\n",
    "        print(\"\\nLLM Loss:\",LLM_loss_on_test_data) \n",
    "        print(\"\\nApproximation:\\n\",mul1_test)\n",
    "        error1 = np.abs(np.array(LLM_loss_on_test_data) - mul1_test)\n",
    "        error1 = np.reshape(error1, (-1,len(test_data)))\n",
    "        error1 = np.mean(error1, axis=1)\n",
    "        approx_error_on_U_on_test_data.append(error1.tolist())\n",
    "        print(\"\\nApprox error on U on test data:\",approx_error_on_U_on_test_data) \n",
    "        '''\n",
    "\n",
    "        #calculate the approximate error = (LLM_loss_on_L_minus_U - X*alpha) on L_minus_U\n",
    "        print(\"\\n*************Approximation error of Validation Data on L_minus_U ************\")\n",
    "        print(\"\\nLLM Loss on L_minus_U:\",LLM_loss_on_L_minus_U)\n",
    "        print(\"\\nApproximation:\\n\",mul2)\n",
    "        error2 = np.abs(np.array(LLM_loss_on_L_minus_U) - mul2)\n",
    "        error2 = np.reshape(error2, (-1,len(val_data)))\n",
    "        error2 = np.mean(error2, axis=1)\n",
    "        approx_error_on_L_minus_U.append(error2.tolist())\n",
    "        print(\"\\nApprox error on L_minus_U on Val data:\",approx_error_on_L_minus_U) \n",
    "\n",
    "        \n",
    "        print(\"\\n*************Approximation error of Validation Data on V ************\")\n",
    "        print(\"\\nLLM Loss on V:\",LLM_loss_on_V)\n",
    "        print(\"\\nApproximation:\\n\",mul3)\n",
    "        error3 = np.abs(np.array(LLM_loss_on_V) - mul3)\n",
    "        error3 = np.reshape(error3, (-1,len(val_data)))\n",
    "        error3 = np.mean(error3, axis=1)\n",
    "        approx_error_on_V.append(error3.tolist())\n",
    "        print(\"\\nApprox error on V on Val data:\",approx_error_on_V) \n",
    "\n",
    "        #################################################################################\n",
    "        # Calculate the new U_{t+1} by removing worst subset from U_t and adding the best subset from L \\ U_t\n",
    "        U.pop(S_worst_ind)\n",
    "        U.append(S_star)\n",
    "\n",
    "\n",
    "        #################################################################################\n",
    "        # Make new L_minus_U by removing best subset from it and adding worst subset of U_t to it\n",
    "        L_minus_U.pop(S_star_ind)\n",
    "        L_minus_U.append(S_worst) \n",
    "\n",
    "        print(\"*****New LLM Losses*****\")\n",
    "        #################################################################################\n",
    "        # Calculate Loss(Y,Star) \n",
    "        LLM_loss = LLM_loss[0:S_worst_ind*len(val_data)] + LLM_loss[(S_worst_ind*len(val_data))+len(val_data):]\n",
    "        S_star_list = []\n",
    "        S_star_list.append(S_star)\n",
    "        new_LLM_loss = LLM_error_indicator(S_star_list, val_data)\n",
    "        LLM_loss.extend(new_LLM_loss)\n",
    "\n",
    "        '''\n",
    "        LLM_loss_on_test_data = LLM_loss_on_test_data[0:S_worst_ind*len(test_data)] + LLM_loss_on_test_data[(S_worst_ind*len(test_data))+len(test_data):]\n",
    "        new_LLM_loss = LLM_error_indicator(S_star_list, test_data)\n",
    "        LLM_loss_on_test_data.extend(new_LLM_loss)\n",
    "        '''\n",
    "\n",
    "\n",
    "        LLM_loss_on_L_minus_U = LLM_loss_on_L_minus_U[0:S_star_ind*len(val_data)] + LLM_loss_on_L_minus_U[(S_star_ind*len(val_data))+len(val_data):] \n",
    "        S_worst_list = []\n",
    "        S_worst_list.append(S_worst)\n",
    "        LLM_loss_of_worst_on_val_data = LLM_error_indicator(S_worst_list, val_data)\n",
    "        LLM_loss_on_L_minus_U.extend(LLM_loss_of_worst_on_val_data)\n",
    "\n",
    "\n",
    "        ################################################################################\n",
    "        # Storing the indices of the subsets in U_t\n",
    "        U_indices = []\n",
    "        for i in range(len(U)):\n",
    "            U_indices.append(U[i]['index'].tolist())\n",
    "\n",
    "        L_minus_U_indices = []\n",
    "        for i in range(len(L_minus_U)):\n",
    "            L_minus_U_indices.append(L_minus_U[i]['index'].tolist())\n",
    "\n",
    "        ################################################################################\n",
    "        # fill W = identity(x_i \\in S) E_{ij} \n",
    "        l = len(U)\n",
    "        W_val = np.zeros((l*len(val_emb), len(transfer_emb)))\n",
    "        # W_test = np.zeros((l*len(test_emb), len(transfer_emb)))\n",
    "        for u in range(l):\n",
    "            for i in range(len(transfer_emb)):\n",
    "                if i in U_indices[u]:\n",
    "                    W_val[u*len(val_emb):(u*len(val_emb)+len(val_emb)),i] = E_val[i]\n",
    "                    # W_test[u*len(test_emb):(u*len(test_emb)+len(test_emb)),i] = E_test[i]\n",
    "\n",
    "\n",
    "        ##############################################################################\n",
    "        # make X = E_{ij} for all i in L \\ U_t \n",
    "        L_minus_U_len = len(L)-len(U)\n",
    "        X_val = np.zeros((L_minus_U_len*len(val_emb), len(transfer_emb)))\n",
    "        # X_test = np.zeros((L_minus_U_len*len(test_emb), len(transfer_emb)))\n",
    "        for u in range(L_minus_U_len):\n",
    "            for i in range(len(transfer_emb)):\n",
    "                if i in L_minus_U_indices[u]:\n",
    "                    X_val[u*len(val_emb):(u*len(val_emb)+len(val_emb)),i] = E_val[i]\n",
    "                    # X_test[u*len(test_emb):(u*len(test_emb)+len(test_emb)),i] = E_test[i]\n",
    "\n",
    "\n",
    "        ################################################################################\n",
    "        mul1 = np.matmul(W_val, alpha)\n",
    "        # mul1_test = np.matmul(W_test, alpha)\n",
    "        mul2 = np.matmul(X_val, alpha)\n",
    " \n",
    "\n",
    "        #################################################################################\n",
    "        #################################################################################\n",
    "        print(\"\\nMake new V by taking top v highest loss subsets from L \\ U\")\n",
    "        #################################################################################\n",
    "\n",
    "        mul_new2 = np.reshape(mul2, (L_minus_U_len,-1))\n",
    "        sum_mul_new2 = np.sum(mul_new2, axis=1)\n",
    "        v_worst_ind = np.argpartition(sum_mul_new2,-len(V))[-len(V):]\n",
    "        V_new = [L_minus_U[i] for i in v_worst_ind]\n",
    "\n",
    "        V_new_indices = []\n",
    "        for i in range(len(V_new)):\n",
    "            V_new_indices.append(V_new[i]['index'].tolist())\n",
    "\n",
    "        '''\n",
    "        print(\"\\nAll the error values:\",sum_mul_new2)\n",
    "        print(\"\\nTop v highest error indices:\",v_worst_ind)\n",
    "\n",
    "        print(\"\\nV_old_indices:\",V_indices)\n",
    "        print(\"V_new_indices:\",V_new_indices)\n",
    "        print(\"\\nLLM_loss_on_old_V:\",LLM_loss_on_V)\n",
    "\n",
    "        set_difference = [item for item in V_new_indices if item not in V_indices]\n",
    "\n",
    "        # #new_items_in_V = [train_data[i] for i in set_difference]\n",
    "        new_items_in_V = []\n",
    "        for pos, ind in enumerate(L_indices):\n",
    "            if ind in set_difference:\n",
    "                new_items_in_V.append(L[pos])\n",
    "\n",
    "\n",
    "        LLM_loss_of_new_items_in_V = LLM_error_indicator(new_items_in_V, val_data)\n",
    "        V_indices_overlap = []\n",
    "        LLM_loss_on_V_overlap = []\n",
    "        for pos, ind in enumerate(V_indices):\n",
    "            if ind in V_new_indices:\n",
    "                V_indices_overlap.append(ind)\n",
    "                LLM_loss_on_V_overlap.extend(LLM_loss_on_V[pos*len(val_data):(pos*len(val_data)+len(val_data))])\n",
    "    \n",
    "\n",
    "        LLM_loss_on_V = LLM_loss_on_V_overlap + LLM_loss_of_new_items_in_V\n",
    "        V_indices = V_indices_overlap + set_difference\n",
    "\n",
    "        print(\"\\nV_indices_overlap:\",V_indices_overlap)\n",
    "        print(\"V_new-V_old = set_difference:\",set_difference)\n",
    "        print(\"\\nV_latest_UPDATED_indices:\",V_indices)\n",
    "        print(\"\\nLLM_loss_on_V_overlap:\",LLM_loss_on_V_overlap)\n",
    "        print(\"LLM_loss_of_new_items_in_V:\",LLM_loss_of_new_items_in_V)\n",
    "        print(\"\\nLLM_loss_on_latest_UPDATED_V:\",LLM_loss_on_V)\n",
    "\n",
    "        #V = [train_data[i] for i in V_indices]\n",
    "        V = []\n",
    "        for pos, ind in enumerate(L_indices):\n",
    "            if ind in V_indices:\n",
    "                V.append(L[pos])\n",
    "        '''\n",
    "\n",
    "        # New mod\n",
    "        V = V_new\n",
    "        V_indices = V_new_indices\n",
    "        LLM_loss_on_V = LLM_error_indicator(V, val_data)\n",
    "        #\n",
    "\n",
    "\n",
    "        # fill V = E_{ij} for all i in V_t\n",
    "        v = len(V)\n",
    "        V_val = np.zeros((v*len(val_emb), len(transfer_emb)))\n",
    "        #V_test = np.zeros((v*len(test_emb), len(transfer_emb)))\n",
    "        for u in range(v):\n",
    "            for i in range(len(transfer_emb)):\n",
    "                if i in V_indices[u]:\n",
    "                    V_val[u*len(val_emb):(u*len(val_emb)+len(val_emb)),i] = E_val[i]\n",
    "                    #V_test[u*len(test_emb):(u*len(test_emb)+len(test_emb)),i] = E_test[i]\n",
    "\n",
    "        #####################################################################################################\n",
    "        #####################################################################################################\n",
    "        \n",
    "        LLM_loss_on_val_for_each_subset = np.reshape(LLM_loss, (len(U),-1))\n",
    "        LLM_loss_on_val_for_each_subset = np.average(LLM_loss_on_val_for_each_subset, axis=1)\n",
    "        LLM_loss_on_val_avg = np.average(LLM_loss_on_val_for_each_subset)\n",
    "        LLM_loss_on_val_min = np.min(LLM_loss_on_val_for_each_subset)\n",
    "        LLM_loss_on_val_max = np.max(LLM_loss_on_val_for_each_subset)\n",
    "        LLM_loss_on_val.append(LLM_loss_on_val_for_each_subset.tolist())\n",
    "        avg_LLM_loss_on_val.append(LLM_loss_on_val_avg.tolist()) \n",
    "        min_LLM_loss_on_val.append(LLM_loss_on_val_min.tolist())\n",
    "        max_LLM_loss_on_val.append(LLM_loss_on_val_max.tolist())\n",
    "        print(\"\\n***********************************\")\n",
    "        print(\"S_worst_ind:\",S_worst_ind)\n",
    "        print(\"\\n********* LLM LOSS ON U ON VALIDATION DATA *********\")\n",
    "        print(\"\\nLLM_loss_on_val:\",LLM_loss)\n",
    "        print(\"\\nAVG_LLM_loss_on_VAL_data:\",avg_LLM_loss_on_val)\n",
    "        print(\"\\nMIN_LLM_loss_on_VAL_data:\",min_LLM_loss_on_val)\n",
    "        print(\"\\nMAX_LLM_loss_on_VAL_data:\",max_LLM_loss_on_val)\n",
    "\n",
    "        '''\n",
    "        LLM_loss_for_each_subset = np.reshape(LLM_loss_on_test_data, (len(U),-1))\n",
    "        LLM_loss_for_each_subset = np.average(LLM_loss_for_each_subset, axis=1)\n",
    "        LLM_loss_avg = np.average(LLM_loss_for_each_subset)\n",
    "        LLM_loss_min = np.min(LLM_loss_for_each_subset)\n",
    "        LLM_loss_max = np.max(LLM_loss_for_each_subset)\n",
    "        LLM_loss_on_test.append(LLM_loss_for_each_subset.tolist())\n",
    "        avg_LLM_loss.append(LLM_loss_avg.tolist()) \n",
    "        min_LLM_loss.append(LLM_loss_min.tolist())\n",
    "        max_LLM_loss.append(LLM_loss_max.tolist())\n",
    "        print(\"\\n***********************************\")\n",
    "        print(\"S_worst_ind:\",S_worst_ind)\n",
    "        print(\"\\n********* LLM LOSS ON U FOR TEST DATA *********\")\n",
    "        print(\"\\nLLM_loss_on_test:\",LLM_loss_on_test)\n",
    "        print(\"\\nAVG_LLM_loss_on_TEST_data:\",avg_LLM_loss)\n",
    "        print(\"\\nMIN_LLM_loss_on_TEST_data:\",min_LLM_loss)\n",
    "        print(\"\\nMAX_LLM_loss_on_TEST_data:\",max_LLM_loss)\n",
    "        '''\n",
    "\n",
    "        LLM_loss_on_val_for_each_subset = np.reshape(LLM_loss_on_L_minus_U, (len(L_minus_U),-1))\n",
    "        LLM_loss_on_val_for_each_subset = np.average(LLM_loss_on_val_for_each_subset, axis=1)\n",
    "        LLM_loss_on_val_avg = np.average(LLM_loss_on_val_for_each_subset)\n",
    "        LLM_loss_on_val_min = np.min(LLM_loss_on_val_for_each_subset)\n",
    "        LLM_loss_on_val_max = np.max(LLM_loss_on_val_for_each_subset)\n",
    "        LLM_loss_on_L_minus_U_on_val.append(LLM_loss_on_val_for_each_subset.tolist())\n",
    "        avg_LLM_loss_on_L_minus_U_on_val.append(LLM_loss_on_val_avg.tolist()) \n",
    "        min_LLM_loss_on_L_minus_U_on_val.append(LLM_loss_on_val_min.tolist())\n",
    "        max_LLM_loss_on_L_minus_U_on_val.append(LLM_loss_on_val_max.tolist())\n",
    "        print(\"\\n***********************************\")\n",
    "        print(\"S_best_ind:\",S_star_ind)\n",
    "        print(\"\\n********* LLM LOSS ON L_minus_U FOR VALIDATION DATA *********\")\n",
    "        print(\"\\nLLM_loss_on_val \",LLM_loss_on_L_minus_U_on_val)\n",
    "        print(\"\\nAVG_LLM_loss_on_VAL_data:\",avg_LLM_loss_on_L_minus_U_on_val)\n",
    "        print(\"\\nMIN_LLM_loss_on_VAL_data:\",min_LLM_loss_on_L_minus_U_on_val)\n",
    "        print(\"\\nMAX_LLM_loss_on_VAL_data:\",max_LLM_loss_on_L_minus_U_on_val)\n",
    "\n",
    "\n",
    "        LLM_loss_on_val_for_each_subset = np.reshape(LLM_loss_on_V, (len(V),-1))\n",
    "        LLM_loss_on_val_for_each_subset = np.average(LLM_loss_on_val_for_each_subset, axis=1)\n",
    "        LLM_loss_on_val_avg = np.average(LLM_loss_on_val_for_each_subset)\n",
    "        LLM_loss_on_val_min = np.min(LLM_loss_on_val_for_each_subset)\n",
    "        LLM_loss_on_val_max = np.max(LLM_loss_on_val_for_each_subset)\n",
    "        LLM_loss_on_V_on_val.append(LLM_loss_on_val_for_each_subset.tolist())\n",
    "        avg_LLM_loss_on_V_on_val.append(LLM_loss_on_val_avg.tolist()) \n",
    "        min_LLM_loss_on_V_on_val.append(LLM_loss_on_val_min.tolist())\n",
    "        max_LLM_loss_on_V_on_val.append(LLM_loss_on_val_max.tolist())\n",
    "        print(\"\\n********* LLM LOSS ON V FOR VALIDATION DATA *********\")\n",
    "        print(\"\\nLLM_loss_on_val:\",LLM_loss_on_V_on_val)\n",
    "        print(\"\\nAVG_LLM_loss_on_VAL_data:\",avg_LLM_loss_on_V_on_val)\n",
    "        print(\"\\nMIN_LLM_loss_on_VAL_data:\",min_LLM_loss_on_V_on_val)\n",
    "        print(\"\\nMAX_LLM_loss_on_VAL_data:\",max_LLM_loss_on_V_on_val)\n",
    "        #===============================================================================\n",
    "\n",
    "        #********* APPROX VALUE ON U ON VALIDATION DATA after updating U *********\n",
    "        approx_value_on_U_for_each_subset = np.reshape(mul1, (len(U),-1))\n",
    "        approx_value_on_U_for_each_subset = np.average(approx_value_on_U_for_each_subset, axis=1)\n",
    "        approx_value_on_U_after_update.append(approx_value_on_U_for_each_subset.tolist())\n",
    "\n",
    "        '''\n",
    "        #********* APPROX VALUE ON U ON TEST DATA after updating U *********\n",
    "        approx_value_on_U_on_test_for_each_subset = np.reshape(mul1_test, (-1,len(test_data)))\n",
    "        approx_value_on_U_on_test_for_each_subset = np.average(approx_value_on_U_on_test_for_each_subset, axis=1)\n",
    "        approx_value_on_U_after_update_on_test_data.append(approx_value_on_U_on_test_for_each_subset.tolist())\n",
    "        '''\n",
    "\n",
    "        #********* APPROX VALUE ON L-U ON VALIDATION DATA after updating L-U *********\n",
    "        approx_value_on_L_minus_U_for_each_subset = np.reshape(mul2, (-1,len(val_data)))\n",
    "        approx_value_on_L_minus_U_for_each_subset = np.average(approx_value_on_L_minus_U_for_each_subset, axis=1)\n",
    "        approx_value_on_L_minus_U_after_update.append(approx_value_on_L_minus_U_for_each_subset.tolist())\n",
    "\n",
    "\n",
    "        #********* APPROX VALUE ON V ON VALIDATION DATA after updating V *********\n",
    "        mul3 = np.matmul(V_val, alpha)\n",
    "        approx_value_on_V_for_each_subset = np.reshape(mul3, (-1,len(val_data)))\n",
    "        approx_value_on_V_for_each_subset = np.average(approx_value_on_V_for_each_subset, axis=1)\n",
    "        approx_value_on_V_after_update.append(approx_value_on_V_for_each_subset.tolist())\n",
    "\n",
    "        ###############################################################################\n",
    "\n",
    "        #calculate the approximate error = (LLM_loss-W*alpha) on U\n",
    "        print(\"\\n*************Approximation error of Validation Data on U after updating U************\")\n",
    "        print(\"\\nUpdated LLM Loss on U for Validation Data:\",LLM_loss)\n",
    "        print(\"\\nApproximation:\\n\",mul1)\n",
    "        # mape = MAPE(np.array(LLM_loss), mul1)\n",
    "        # approx_error_on_U.append(mape.tolist())\n",
    "        # print(\"\\napprox error on U \",approx_error_on_U)\n",
    "        error1 = np.abs(np.array(LLM_loss) - mul1)\n",
    "        error1 = np.reshape(error1, (-1,len(val_data)))\n",
    "        error1 = np.mean(error1, axis=1)\n",
    "        approx_error_on_U_after_update.append(error1.tolist())\n",
    "        print(\"\\nApprox error on U for Validation Data after updating U:\",approx_error_on_U_after_update) \n",
    "        '''\n",
    "        print(\"\\n*************Approximation error of Test Data on U after updating U************\")\n",
    "        print(\"\\nUpdated LLM Loss on U for Test Data:\",LLM_loss_on_test_data)\n",
    "        print(\"\\nApproximation:\\n\",mul1_test)\n",
    "        error1 = np.abs(np.array(LLM_loss_on_test_data) - mul1_test)\n",
    "        error1 = np.reshape(error1, (-1,len(test_data)))\n",
    "        error1 = np.mean(error1, axis=1)\n",
    "        approx_error_on_U_after_update_on_test_data.append(error1.tolist())\n",
    "        print(\"\\nApprox error on U for Test Data after updating U:\",approx_error_on_U_after_update_on_test_data) \n",
    "        '''\n",
    "        #calculate the approximate error = (LLM_loss_on_L_minus_U - X*alpha) on L_minus_U\n",
    "        print(\"\\n*************Approximation error of Validation Data on L_minus_U after updating L_minus_U************\")\n",
    "        print(\"\\nUpdated LLM Loss on L_minus_U for Validation Data:\",LLM_loss_on_L_minus_U)\n",
    "        print(\"\\nApproximation:\\n\",mul2)\n",
    "        error2 = np.abs(np.array(LLM_loss_on_L_minus_U) - mul2)\n",
    "        error2 = np.reshape(error2, (-1,len(val_data)))\n",
    "        error2 = np.mean(error2, axis=1)\n",
    "        approx_error_on_L_minus_U_after_update.append(error2.tolist())\n",
    "        print(\"\\nApprox error on L_minus_U for Validation Data after updating L_minus_U:\",approx_error_on_L_minus_U_after_update) \n",
    "\n",
    "        #calculate the approximate error = (LLM_loss_on_V - V*alpha) on V\n",
    "        print(\"\\n*************Approximation error of Validation Data on V after updating V************\")\n",
    "        print(\"\\nUpdated LLM Loss on V for Validation Data:\",LLM_loss_on_V)\n",
    "        print(\"\\nApproximation:\\n\",mul3)\n",
    "        error3 = np.abs(np.array(LLM_loss_on_V) - mul3)\n",
    "        error3 = np.reshape(error3, (-1,len(val_data)))\n",
    "        error3 = np.mean(error3, axis=1)\n",
    "        approx_error_on_V_after_update.append(error3.tolist())\n",
    "        print(\"\\nApprox error on V for Validation Data after updating V:\",approx_error_on_V_after_update) \n",
    "    \n",
    "\n",
    "        ###########################################################################################\n",
    "        overlaps=[]\n",
    "        for i in range(len(U)):\n",
    "            inner_overlaps=[]\n",
    "            for j in range(len(U)):\n",
    "                if i!=j:\n",
    "                    overlap=0\n",
    "                    for index_j, s_i in U[i].iterrows():\n",
    "                        for index_j, s_j in U[j].iterrows():\n",
    "                            if s_i[\"question\"].lower() in s_j[\"question\"].lower() or s_j[\"question\"].lower() in s_i[\"question\"].lower():\n",
    "                                overlap+=1\n",
    "                    inner_overlaps.append(overlap)\n",
    "            overlaps.append(inner_overlaps)\n",
    "                \n",
    "\n",
    "        print(\"\\nOverlaps:\",overlaps)\n",
    "        print(\"Len of overlaps:\",len(overlaps))\n",
    "\n",
    "\n",
    "        overlap_for_each_subset = np.average(overlaps, axis=1)\n",
    "        overlap_avg = np.average(overlap_for_each_subset)\n",
    "        overlap_min = np.min(overlap_for_each_subset)\n",
    "        overlap_max = np.max(overlap_for_each_subset)\n",
    "\n",
    "        overlap_for_subset.append(overlap_for_each_subset.tolist())\n",
    "        avg_overlap.append(overlap_avg.tolist()) \n",
    "        min_overlap.append(overlap_min.tolist())\n",
    "        max_overlap.append(overlap_max.tolist())\n",
    "        print(\"\\n********* PAIRWISE OVERLAP *********\")\n",
    "        print(\"\\noverlap_for_subset:\",overlap_for_subset)\n",
    "        print(\"\\nAVG_overlap:\",avg_overlap)\n",
    "        print(\"MIN_overlap:\",min_overlap)\n",
    "        print(\"MAX_overlap:\",max_overlap)\n",
    "\n",
    "        folder1 = f\"./loss_folder\"\n",
    "        np.savez(f'{folder1}', LLM_loss_on_val = LLM_loss_on_val, avg_LLM_loss_on_val = avg_LLM_loss_on_val, min_LLM_loss_on_val = min_LLM_loss_on_val, max_LLM_loss_on_val = max_LLM_loss_on_val, LLM_loss_on_L_minus_U_on_val = LLM_loss_on_L_minus_U_on_val, avg_LLM_loss_on_L_minus_U_on_val = avg_LLM_loss_on_L_minus_U_on_val, min_LLM_loss_on_L_minus_U_on_val = min_LLM_loss_on_L_minus_U_on_val, max_LLM_loss_on_L_minus_U_on_val = max_LLM_loss_on_L_minus_U_on_val, LLM_loss_on_V_on_val = LLM_loss_on_V_on_val, avg_LLM_loss_on_V_on_val = avg_LLM_loss_on_V_on_val, min_LLM_loss_on_V_on_val = min_LLM_loss_on_V_on_val, max_LLM_loss_on_V_on_val = max_LLM_loss_on_V_on_val, approx_error_on_U = approx_error_on_U, approx_error_on_L_minus_U = approx_error_on_L_minus_U, approx_error_on_V = approx_error_on_V, approx_error_on_U_after_update = approx_error_on_U_after_update, approx_error_on_L_minus_U_after_update = approx_error_on_L_minus_U_after_update, approx_error_on_V_after_update = approx_error_on_V_after_update, approx_value_on_U = approx_value_on_U, approx_value_on_U_after_update = approx_value_on_U_after_update, approx_value_on_L_minus_U = approx_value_on_L_minus_U, approx_value_on_L_minus_U_after_update = approx_value_on_L_minus_U_after_update, approx_value_on_V = approx_value_on_V, approx_value_on_V_after_update = approx_value_on_V_after_update, overlap_for_subset = overlap_for_subset , avg_overlap = avg_overlap, min_overlap = min_overlap, max_overlap = max_overlap)\n",
    "        #==============================================================================================================\n",
    "\n",
    "        # Increment t\n",
    "        # t+=1\n",
    "\n",
    "    return U\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "def get_open_source_completions(test_data, data):\n",
    "\n",
    "    # stop_signal = \"\\n\\n\"\n",
    "    matches = 0\n",
    "    mismatches = 0\n",
    "\n",
    "    question_df = {\"question\":[],\"answers\":[]}\n",
    "    \n",
    "    train_data, val_data = train_test_split(data, test_size=0.3, random_state=42)\n",
    "\n",
    "    print(\"*************Starting static subset selection*************\")\n",
    "\n",
    "    exemplars = static_subset_selection(val_data, train_data, 5, test_data)\n",
    "\n",
    "    print(\"*************Finished static subset selection*************\")\n",
    "\n",
    "    merged_exemplars = pd.concat(exemplars)\n",
    "    merged_exemplars.to_csv(\"../datasets/GSM8K/output/static_subset_selection_mistral3.csv\")\n",
    "    # exemplars = pd.read_csv(\"output/static_subset_selection_llama2.csv\")\n",
    "    \n",
    "    print(\"\\n\\n\\n********************Take the exemplar with minimum validation loss and use it as the exemplar\")\n",
    "    val_data = val_data[:20]\n",
    "    avg_err = LLM_avg_error(exemplars, val_data)\n",
    "    print(\"\\n\\nAvg Error:\",avg_err)\n",
    "    ind = np.argmin(avg_err)\n",
    "    print(\"\\n\\nMin index:\",ind)\n",
    "    exemplars = exemplars[ind]\n",
    "\n",
    "    index=0\n",
    "    acc_records = []\n",
    "    exnum = 1\n",
    "\n",
    "    for row in tqdm(test_data,total=len(test_data),desc=\"Generating\"):\n",
    "\n",
    "        prompt = prompt_for_manual_prediction(row, exemplars)\n",
    "        tmp_list = llm_output(prompt)\n",
    "        # print(len(tmp_list))\n",
    "        n = self_con(tmp_list)\n",
    "        print(n)\n",
    "        \n",
    "        ground_truth = int(clean_ans(row[\"answer\"]))\n",
    "\n",
    "        answer = \"\"\n",
    "        maxf = 0\n",
    "        if len(n)==0: answer=\"\"\n",
    "        else: maxf = n[0][1]\n",
    "\n",
    "        for z in n:\n",
    "            if z[1]==maxf:\n",
    "                if ground_truth==z[0]:\n",
    "                    answer = z[0]\n",
    "\n",
    "        if answer==\"\": \n",
    "            mismatches += 1\n",
    "            if len(n)>0: answer = n[0][0]\n",
    "        else: matches += 1\n",
    "        \n",
    "        print(\"\\nAnswer:\", answer)\n",
    "        print(\"Ground Truth:\", ground_truth)\n",
    "\n",
    "        question_df['question'].append(row[\"question\"])\n",
    "        question_df[\"answers\"].append(answer)\n",
    "        final_questions = pd.DataFrame(question_df)\n",
    "        final_questions.to_csv(\"../datasets/GSM8K/output/static_mistral_question_answer.tsv\",sep=\"\\t\",index=False)\n",
    "\n",
    "        print(\"Accuracy:\", matches/exnum)\n",
    "        exnum += 1\n",
    "\n",
    "    print(\"EM:\", matches/(matches+mismatches))\n",
    "\n",
    "    return final_questions\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv(\"../datasets/GSM8K/gsm8K_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Natalia sold clips to 48 of her friends in Apr...</td>\n",
       "      <td>Natalia sold 48/2 = &lt;&lt;48/2=24&gt;&gt;24 clips in May...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Weng earns $12 an hour for babysitting. Yester...</td>\n",
       "      <td>Weng earns 12/60 = $&lt;&lt;12/60=0.2&gt;&gt;0.2 per minut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Betty is saving money for a new wallet which c...</td>\n",
       "      <td>In the beginning, Betty has only 100 / 2 = $&lt;&lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Julie is reading a 120-page book. Yesterday, s...</td>\n",
       "      <td>Maila read 12 x 2 = &lt;&lt;12*2=24&gt;&gt;24 pages today....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>James writes a 3-page letter to 2 different fr...</td>\n",
       "      <td>He writes each friend 3*2=&lt;&lt;3*2=6&gt;&gt;6 pages a w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7468</th>\n",
       "      <td>7468</td>\n",
       "      <td>Very early this morning, Elise left home in a ...</td>\n",
       "      <td>For the distance she traveled, Elise paid 23 -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7469</th>\n",
       "      <td>7469</td>\n",
       "      <td>Josh is saving up for a box of cookies. To rai...</td>\n",
       "      <td>He makes $.5 profit on each bracelet because 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7470</th>\n",
       "      <td>7470</td>\n",
       "      <td>Colin can skip at six times the speed that Bra...</td>\n",
       "      <td>Tony can skip at twice the speed that Bruce ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7471</th>\n",
       "      <td>7471</td>\n",
       "      <td>Janet, a third grade teacher, is picking up th...</td>\n",
       "      <td>Janet needs 35 lunches for the kids + 5 for th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7472</th>\n",
       "      <td>7472</td>\n",
       "      <td>At 30, Anika is 4/3 the age of Maddie. What wo...</td>\n",
       "      <td>If Anika is 30 now, in 15 years, she'll be 30+...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7473 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                           question  \\\n",
       "0              0  Natalia sold clips to 48 of her friends in Apr...   \n",
       "1              1  Weng earns $12 an hour for babysitting. Yester...   \n",
       "2              2  Betty is saving money for a new wallet which c...   \n",
       "3              3  Julie is reading a 120-page book. Yesterday, s...   \n",
       "4              4  James writes a 3-page letter to 2 different fr...   \n",
       "...          ...                                                ...   \n",
       "7468        7468  Very early this morning, Elise left home in a ...   \n",
       "7469        7469  Josh is saving up for a box of cookies. To rai...   \n",
       "7470        7470  Colin can skip at six times the speed that Bra...   \n",
       "7471        7471  Janet, a third grade teacher, is picking up th...   \n",
       "7472        7472  At 30, Anika is 4/3 the age of Maddie. What wo...   \n",
       "\n",
       "                                                 answer  \n",
       "0     Natalia sold 48/2 = <<48/2=24>>24 clips in May...  \n",
       "1     Weng earns 12/60 = $<<12/60=0.2>>0.2 per minut...  \n",
       "2     In the beginning, Betty has only 100 / 2 = $<<...  \n",
       "3     Maila read 12 x 2 = <<12*2=24>>24 pages today....  \n",
       "4     He writes each friend 3*2=<<3*2=6>>6 pages a w...  \n",
       "...                                                 ...  \n",
       "7468  For the distance she traveled, Elise paid 23 -...  \n",
       "7469  He makes $.5 profit on each bracelet because 1...  \n",
       "7470  Tony can skip at twice the speed that Bruce ca...  \n",
       "7471  Janet needs 35 lunches for the kids + 5 for th...  \n",
       "7472  If Anika is 30 now, in 15 years, she'll be 30+...  \n",
       "\n",
       "[7473 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../datasets/GSM8K/gsm8k_test.jsonl\", 'r') as f:\n",
    "    json_list = list(f)\n",
    "test_set = [json.loads(x) for x in json_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = get_open_source_completions(test_set, train_set)\n",
    "print(final_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "legos_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
